{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "%matplotlib qt5\n",
    "import pylab as plb\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import mountaincar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DummyAgent():\n",
    "    \"\"\"A not so good agent for the mountain-car task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mountain_car = None, x_linspace = (-150, 30, 20),\n",
    "                v_linspace = (-15, 15, 20), w = None, tau = 1, gamma = 0.95,\n",
    "                 eta = 0.01, lambda_ = 0.5):\n",
    "        ''' Initialize the object '''\n",
    "        \n",
    "        # saving the environment object\n",
    "        if mountain_car is None:\n",
    "            self.mountain_car = mountaincar.MountainCar()\n",
    "        else:\n",
    "            self.mountain_car = mountain_car\n",
    "        \n",
    "        # range for x neurons grid\n",
    "        self.x_values = np.linspace(*x_linspace)\n",
    "\n",
    "        # range for v neurons grid\n",
    "        self.v_values = np.linspace(*v_linspace)\n",
    "\n",
    "        # steps x and v\n",
    "        self.delta_x = self.x_values[1] - self.x_values[0]\n",
    "        self.delta_v = self.v_values[1] - self.v_values[0]\n",
    "\n",
    "        # sigmas x and v\n",
    "        self.sigma_x = np.array([self.delta_x] * len(self.x_values))\n",
    "        self.sigma_v = np.array([self.delta_v] * len(self.v_values))\n",
    "\n",
    "        # number of actions\n",
    "        self.n_actions = 3\n",
    "\n",
    "        # number of neurons\n",
    "        self.n_neurons = len(self.x_values) * len(self.v_values)\n",
    "\n",
    "        # weight matrix\n",
    "        if w is None:\n",
    "            self.w = np.random.randn(self.n_actions, self.n_neurons)\n",
    "        else:\n",
    "            self.w = w\n",
    "\n",
    "        # sampling softmax temperature\n",
    "        self.tau = tau\n",
    "        \n",
    "        # reward discount factor\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # eligibility trace parameter\n",
    "        self.lambda_ = lambda_\n",
    "            \n",
    "    def r(self, x, v):\n",
    "        ''' get neuron activations for s = (x, v) '''\n",
    "        # x in rows, v in columns\n",
    "        part_x = np.reshape(np.divide((self.x_values - x) ** 2, self.sigma_x ** 2), (-1, 1))\n",
    "        part_v = np.reshape(np.divide((self.v_values - v) ** 2, self.sigma_v ** 2), (1, -1))\n",
    "        return np.exp(-(part_x + part_v))\n",
    "\n",
    "    def get_Q(self, x, v):\n",
    "        ''' Get Q-function at given s = (x, v) with weights w '''\n",
    "        \n",
    "        return np.reshape(self.w @ np.reshape(self.r(x, v), (-1, 1)), (-1,))\n",
    "\n",
    "    def get_action_probas(self, Q):\n",
    "        ''' get action probabilities as a vector '''\n",
    "\n",
    "        vector = np.exp(Q / self.tau)\n",
    "        return vector / np.sum(vector)\n",
    "\n",
    "    def get_action_index(self, x, v):\n",
    "        ''' Sample action for s = (x, v) and weights w with parameter tau '''\n",
    "\n",
    "        Q = self.get_Q(x, v)\n",
    "        action_probas = self.get_action_probas(Q)\n",
    "        return np.random.choice(range(self.n_actions), p = action_probas)\n",
    "\n",
    "    def update_w(self, x, v, a_index, delta):\n",
    "        ''' Perform gradient descent on Q(s, a) by delta given s and a'''\n",
    "        \n",
    "        dQ_dwa = np.reshape(self.r(x, v), -1)\n",
    "        self.w[a_index, :] += delta * dQ_dwa\n",
    "\n",
    "    def visualize_trial(self, n_steps = 200):\n",
    "        \"\"\"Do a trial without learning, with display.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_steps -- number of steps to simulate for\n",
    "        \"\"\"\n",
    "        \n",
    "        # prepare for the visualization\n",
    "        plb.ion()\n",
    "        mv = mountaincar.MountainCarViewer(self.mountain_car)\n",
    "        mv.create_figure(n_steps, n_steps)\n",
    "        plb.draw()\n",
    "        plb.pause(1e-3)\n",
    "            \n",
    "        # make sure the mountain-car is reset\n",
    "        self.mountain_car.reset()\n",
    "        \n",
    "        for n in (range(n_steps)):\n",
    "            print('\\rt =', s, self.mountain_car.t)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # update the visualization\n",
    "            mv.update_figure()\n",
    "            plb.draw()\n",
    "            plb.pause(1e-3)\n",
    "            \n",
    "            # check for rewards\n",
    "            if self.mountain_car.R > 0.0:\n",
    "                print(\"\\rreward obtained at t = \", self.mountain_car.t)\n",
    "                break\n",
    "\n",
    "    def learn(self, max_steps = 10000):\n",
    "        \"\"\"Do a trial without learning, with display.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_steps -- number of steps to simulate for\n",
    "        \"\"\"\n",
    "            \n",
    "        # make sure the mountain-car is reset\n",
    "        self.mountain_car.reset()\n",
    "\n",
    "        # saved previous state\n",
    "        old_s = None\n",
    "        old_a = None\n",
    "        \n",
    "        # all states and actions array\n",
    "        all_s_a = []\n",
    "        \n",
    "        for n in range(max_steps):\n",
    "            # get current state\n",
    "            s = (self.mountain_car.x, self.mountain_car.x_d)\n",
    "            \n",
    "            #print('\\rt =', s, self.mountain_car.t, self.get_action_index(*s))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # selection current action based on softmax\n",
    "            action_index = self.get_action_index(*s)\n",
    "\n",
    "            # save s, a\n",
    "            all_s_a.append((s, action_index))\n",
    "            \n",
    "            # perform the action\n",
    "            self.mountain_car.apply_force(action_index - 1)\n",
    "            \n",
    "            # simulate the timestep\n",
    "            self.mountain_car.simulate_timesteps(100, 0.01)\n",
    "            \n",
    "            # check for rewards (runs at the end once)\n",
    "            if self.mountain_car.R > 0.0:\n",
    "                # print the obtained reward\n",
    "                print(\"\\rreward obtained at t = \", self.mountain_car.t)\n",
    "                \n",
    "                # compute vector [xi ^ (T-1), ..., 1] where xi = gamma * lambda\n",
    "                eligibility_trace = np.flip(np.array([self.gamma * self.lambda_]) **\n",
    "                                            np.arange(len(all_s_a)), axis = 0)\n",
    "                \n",
    "                # compute the update for the Q function\n",
    "                # update = eta * delta (from lectures)\n",
    "                \n",
    "                # old Q\n",
    "                Q = self.get_Q(*old_s)[old_a]\n",
    "                \n",
    "                # new Q\n",
    "                Q1 = np.max(self.get_Q(*s))\n",
    "                \n",
    "                # eta * (R + gamma * Qnew - Qold)\n",
    "                update = self.eta * (self.mountain_car.R + self.gamma * Q1 - Q)\n",
    "                \n",
    "                # loop over history\n",
    "                i = 0\n",
    "                for s0, a0 in all_s_a:\n",
    "                    # updating Q based on SARSA and eligibility traces\n",
    "                    self.update_w(s0[0], s0[1], a0, update * eligibility_trace[i])\n",
    "                    i += 1\n",
    "                    \n",
    "                # no steps after the reward\n",
    "                break\n",
    "                \n",
    "            # saving old state\n",
    "            old_s = s\n",
    "            old_a = action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward obtained at t =  562.0\n",
      "reward obtained at t =  290.0\n",
      "reward obtained at t =  728.0\n",
      "reward obtained at t =  366.0\n",
      "reward obtained at t =  615.0\n",
      "reward obtained at t =  1366.0\n",
      "reward obtained at t =  226.0\n"
     ]
    }
   ],
   "source": [
    "# create an agent\n",
    "d = DummyAgent(tau = 1)\n",
    "\n",
    "# learn for 100 iterations\n",
    "for i in range(100):\n",
    "    d.learn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
