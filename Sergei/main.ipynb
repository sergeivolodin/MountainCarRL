{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "%matplotlib qt5\n",
    "import pylab as plb\n",
    "import numpy as np\n",
    "import mountaincar\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DummyAgent():\n",
    "    \"\"\"A not so good agent for the mountain-car task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mountain_car = None, x_linspace = (-150, 30, 20),\n",
    "                v_linspace = (-15, 15, 20), w = None, tau = 1, gamma = 0.95,\n",
    "                 eta = 0.0001, lambda_ = 0.95):\n",
    "        ''' Initialize the object '''\n",
    "        \n",
    "        # saving the environment object\n",
    "        if mountain_car is None:\n",
    "            self.mountain_car = mountaincar.MountainCar()\n",
    "        else:\n",
    "            self.mountain_car = mountain_car\n",
    "        \n",
    "        # range for x neurons grid\n",
    "        self.x_values = np.linspace(*x_linspace)\n",
    "\n",
    "        # range for v neurons grid\n",
    "        self.v_values = np.linspace(*v_linspace)\n",
    "\n",
    "        # steps x and v\n",
    "        self.delta_x = self.x_values[1] - self.x_values[0]\n",
    "        self.delta_v = self.v_values[1] - self.v_values[0]\n",
    "\n",
    "        # sigmas x and v\n",
    "        self.sigma_x = np.array([self.delta_x] * len(self.x_values))\n",
    "        self.sigma_v = np.array([self.delta_v] * len(self.v_values))\n",
    "\n",
    "        # number of actions\n",
    "        self.n_actions = 3\n",
    "\n",
    "        # number of neurons\n",
    "        self.n_neurons = len(self.x_values) * len(self.v_values)\n",
    "\n",
    "        # weight matrix\n",
    "        if w is None:\n",
    "            #self.w = np.random.randn(self.n_actions, self.n_neurons)\n",
    "            self.w = np.zeros((self.n_actions, self.n_neurons))\n",
    "        else:\n",
    "            self.w = w\n",
    "\n",
    "        # sampling softmax temperature\n",
    "        self.tau = tau\n",
    "        \n",
    "        # reward discount factor\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # eligibility trace parameter\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def r(self, x, v):\n",
    "        ''' get neuron activations for s = (x, v) '''\n",
    "        # x in rows, v in columns\n",
    "        part_x = np.reshape(np.divide((self.x_values - x) ** 2, self.sigma_x ** 2), (-1, 1))\n",
    "        part_v = np.reshape(np.divide((self.v_values - v) ** 2, self.sigma_v ** 2), (1, -1))\n",
    "        return np.exp(-(part_x + part_v))\n",
    "\n",
    "    def get_Q(self, x, v):\n",
    "        ''' Get Q-function at given s = (x, v) with weights w '''\n",
    "        \n",
    "        return np.reshape(self.w @ np.reshape(self.r(x, v), (-1, 1)), (-1,))\n",
    "    \n",
    "    def get_Q_matrix(self):\n",
    "        ''' Returns matrices indexed by (x, v) with values argmax Q and max Q '''\n",
    "        \n",
    "        result_index = np.zeros((len(self.x_values), len(self.v_values)))\n",
    "        result_value = np.zeros((len(self.x_values), len(self.v_values)))\n",
    "        for i, x in enumerate(self.x_values):\n",
    "            for j, v in enumerate(self.v_values):\n",
    "                Q = self.get_Q(x, v)\n",
    "                result_index[i, j] = np.argmax(Q)\n",
    "                result_value[i, j] = np.max(Q)\n",
    "        return result_index, result_value\n",
    "    \n",
    "    def plot_Q_matrix(self):\n",
    "        ''' Plot vector field action(x, v) '''\n",
    "        \n",
    "        A, V = self.get_Q_matrix()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title('Action selection')\n",
    "        plt.xlabel('$x$')\n",
    "        plt.ylabel('$\\dot{x}$')\n",
    "        plt.xlim((np.min(self.x_values) * 1.1, np.max(self.x_values) * 1.1))\n",
    "        plt.ylim((np.min(self.v_values) * 1.1, np.max(self.v_values) * 1.1))\n",
    "        for i, x in enumerate(self.x_values):\n",
    "            for j, v in enumerate(self.v_values):\n",
    "                action = A[i, j]\n",
    "                value = V[i, j]\n",
    "                if action == 1:\n",
    "                    plt.scatter(x, v, 3, c = 'black')\n",
    "                else:\n",
    "                    direction = action - 1\n",
    "                    plt.arrow(x, v, 3 * direction, 0, head_width=0.5, head_length=1, color = 'red' if direction < 0 else 'green')\n",
    "\n",
    "    def get_action_probas(self, Q):\n",
    "        ''' get action probabilities as a vector '''\n",
    "\n",
    "        vector = np.exp(Q / self.tau)\n",
    "        return vector / np.sum(vector)\n",
    "\n",
    "    def get_action_index(self, x, v):\n",
    "        ''' Sample action for s = (x, v) and weights w with parameter tau '''\n",
    "\n",
    "        Q = self.get_Q(x, v)\n",
    "        action_probas = self.get_action_probas(Q)\n",
    "        return np.random.choice(range(self.n_actions), p = action_probas)\n",
    "\n",
    "    def update_w(self, x, v, a_index, delta):\n",
    "        ''' Perform gradient descent on Q(s, a) by delta given s and a'''\n",
    "        \n",
    "        dQ_dwa = np.reshape(self.r(x, v), -1)\n",
    "        self.w[a_index, :] += delta * dQ_dwa\n",
    "\n",
    "    def visualize_trial(self, n_steps = 200):\n",
    "        \"\"\"Do a trial without learning, with display.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_steps -- number of steps to simulate for\n",
    "        \"\"\"\n",
    "        \n",
    "        # prepare for the visualization\n",
    "        plb.ion()\n",
    "        mv = mountaincar.MountainCarViewer(self.mountain_car)\n",
    "        mv.create_figure(n_steps, n_steps)\n",
    "        plb.draw()\n",
    "        plb.pause(1e-3)\n",
    "            \n",
    "        # make sure the mountain-car is reset\n",
    "        self.mountain_car.reset()\n",
    "        \n",
    "        for n in (range(n_steps)):\n",
    "            print('\\rt =', self.mountain_car.t)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # get current state\n",
    "            s = (self.mountain_car.x, self.mountain_car.x_d)\n",
    "\n",
    "            # selection current action based on softmax\n",
    "            action_index = self.get_action_index(*s)\n",
    "            \n",
    "            # perform the action\n",
    "            self.mountain_car.apply_force(action_index - 1)\n",
    "            \n",
    "            # simulate the timestep\n",
    "            self.mountain_car.simulate_timesteps(100, 0.01)\n",
    "\n",
    "            # update the visualization\n",
    "            mv.update_figure()\n",
    "            plb.draw()\n",
    "            plb.pause(1e-3)\n",
    "            \n",
    "            # check for rewards\n",
    "            if self.mountain_car.R > 0.0:\n",
    "                print(\"\\rreward obtained at t = \", self.mountain_car.t)\n",
    "                break\n",
    "\n",
    "    def learn(self, max_steps = 1500):\n",
    "        \"\"\"Do a trial without learning, with display.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_steps -- number of steps to simulate for\n",
    "        \"\"\"\n",
    "            \n",
    "        # make sure the mountain-car is reset\n",
    "        self.mountain_car.reset()\n",
    "\n",
    "        # saved previous state\n",
    "        old_s = None\n",
    "        old_a = None\n",
    "        \n",
    "        # all states and actions array\n",
    "        all_s_a = []\n",
    "        \n",
    "        # is finished\n",
    "        finished = False\n",
    "        \n",
    "        for n in (range(max_steps)):\n",
    "            # get current state\n",
    "            s = (self.mountain_car.x, self.mountain_car.x_d)\n",
    "            \n",
    "            #print('\\rt =', s, self.mountain_car.t, self.get_action_index(*s))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # selection current action based on softmax\n",
    "            action_index = self.get_action_index(*s)\n",
    "\n",
    "            # save s, a\n",
    "            all_s_a.append((s, action_index))\n",
    "            \n",
    "            # perform the action\n",
    "            self.mountain_car.apply_force(action_index - 1)\n",
    "            \n",
    "            # simulate the timestep\n",
    "            self.mountain_car.simulate_timesteps(100, 0.01)\n",
    "            \n",
    "            # check for rewards (runs at the end once)\n",
    "            if self.mountain_car.R > 0.0:\n",
    "                # print the obtained reward\n",
    "                print(\"\\rreward obtained at t = \", self.mountain_car.t)\n",
    "                \n",
    "                # compute vector [xi ^ (T-1), ..., 1] where xi = gamma * lambda\n",
    "                eligibility_trace = np.flip(np.array([self.gamma * self.lambda_]) **\n",
    "                                            np.arange(len(all_s_a)), axis = 0)\n",
    "                \n",
    "                # compute the update for the Q function\n",
    "                # update = eta * delta (from lectures)\n",
    "                \n",
    "                # old Q\n",
    "                Q = self.get_Q(*old_s)[old_a]\n",
    "                \n",
    "                # new Q\n",
    "                Q1 = np.max(self.get_Q(*s))\n",
    "                \n",
    "                # eta * (R + gamma * Qnew - Qold)\n",
    "                update = self.eta * (self.mountain_car.R + self.gamma * Q1 - Q)\n",
    "                \n",
    "                # loop over history\n",
    "                i = 0\n",
    "                for s0, a0 in all_s_a:\n",
    "                    # updating Q based on SARSA and eligibility traces\n",
    "                    self.update_w(s0[0], s0[1], a0, update * eligibility_trace[i])\n",
    "                    i += 1\n",
    "                    \n",
    "                # no steps after the reward\n",
    "                finished = True\n",
    "                break\n",
    "                \n",
    "            # saving old state\n",
    "            old_s = s\n",
    "            old_a = action_index\n",
    "        if not finished:\n",
    "            print('No reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an agent\n",
    "d = DummyAgent(tau = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward obtained at t =  664.0\n",
      "reward obtained at t =  652.0\n"
     ]
    }
   ],
   "source": [
    "# learn for 100 iterations\n",
    "for i in range(10):\n",
    "    d.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0.0\n",
      "t = 1.0\n",
      "t = 2.0\n",
      "t = 3.0\n",
      "t = 4.0\n",
      "t = 5.0\n",
      "t = 6.0\n",
      "t = 7.0\n",
      "t = 8.0\n",
      "t = 9.0\n",
      "t = 10.0\n",
      "t = 11.0\n",
      "t = 12.0\n",
      "t = 13.0\n",
      "t = 14.0\n",
      "t = 15.0\n",
      "t = 16.0\n",
      "t = 17.0\n",
      "t = 18.0\n",
      "t = 19.0\n",
      "t = 20.0\n",
      "t = 21.0\n",
      "t = 22.0\n",
      "t = 23.0\n",
      "t = 24.0\n",
      "t = 25.0\n",
      "t = 26.0\n",
      "t = 27.0\n",
      "t = 28.0\n",
      "t = 29.0\n",
      "t = 30.0\n",
      "t = 31.0\n",
      "t = 32.0\n",
      "t = 33.0\n",
      "t = 34.0\n",
      "t = 35.0\n",
      "t = 36.0\n",
      "t = 37.0\n",
      "t = 38.0\n",
      "t = 39.0\n",
      "t = 40.0\n",
      "t = 41.0\n",
      "t = 42.0\n",
      "t = 43.0\n",
      "t = 44.0\n",
      "t = 45.0\n",
      "t = 46.0\n",
      "t = 47.0\n",
      "t = 48.0\n",
      "t = 49.0\n",
      "t = 50.0\n",
      "t = 51.0\n",
      "t = 52.0\n",
      "t = 53.0\n",
      "t = 54.0\n",
      "t = 55.0\n",
      "t = 56.0\n",
      "t = 57.0\n",
      "t = 58.0\n",
      "t = 59.0\n",
      "t = 60.0\n",
      "t = 61.0\n",
      "t = 62.0\n",
      "t = 63.0\n",
      "t = 64.0\n",
      "t = 65.0\n",
      "t = 66.0\n",
      "t = 67.0\n",
      "t = 68.0\n",
      "t = 69.0\n",
      "t = 70.0\n",
      "t = 71.0\n",
      "t = 72.0\n",
      "t = 73.0\n",
      "t = 74.0\n",
      "t = 75.0\n",
      "t = 76.0\n",
      "t = 77.0\n",
      "t = 78.0\n",
      "t = 79.0\n",
      "t = 80.0\n",
      "t = 81.0\n",
      "t = 82.0\n",
      "t = 83.0\n",
      "t = 84.0\n",
      "t = 85.0\n",
      "t = 86.0\n",
      "t = 87.0\n",
      "t = 88.0\n",
      "t = 89.0\n",
      "t = 90.0\n",
      "t = 91.0\n",
      "t = 92.0\n",
      "t = 93.0\n",
      "t = 94.0\n",
      "t = 95.0\n",
      "t = 96.0\n",
      "t = 97.0\n",
      "t = 98.0\n",
      "t = 99.0\n",
      "t = 100.0\n",
      "t = 101.0\n",
      "t = 102.0\n",
      "t = 103.0\n",
      "t = 104.0\n",
      "t = 105.0\n",
      "t = 106.0\n",
      "t = 107.0\n",
      "t = 108.0\n",
      "t = 109.0\n",
      "t = 110.0\n",
      "t = 111.0\n",
      "t = 112.0\n",
      "t = 113.0\n",
      "t = 114.0\n",
      "t = 115.0\n",
      "t = 116.0\n",
      "t = 117.0\n",
      "t = 118.0\n",
      "t = 119.0\n",
      "t = 120.0\n",
      "t = 121.0\n",
      "t = 122.0\n",
      "t = 123.0\n",
      "t = 124.0\n",
      "t = 125.0\n",
      "t = 126.0\n",
      "t = 127.0\n",
      "t = 128.0\n",
      "t = 129.0\n",
      "t = 130.0\n",
      "t = 131.0\n",
      "t = 132.0\n",
      "t = 133.0\n",
      "t = 134.0\n",
      "t = 135.0\n",
      "t = 136.0\n",
      "t = 137.0\n",
      "t = 138.0\n",
      "t = 139.0\n",
      "t = 140.0\n",
      "t = 141.0\n",
      "t = 142.0\n",
      "t = 143.0\n",
      "t = 144.0\n",
      "t = 145.0\n",
      "t = 146.0\n",
      "t = 147.0\n",
      "t = 148.0\n",
      "t = 149.0\n",
      "t = 150.0\n",
      "t = 151.0\n",
      "t = 152.0\n",
      "t = 153.0\n",
      "t = 154.0\n",
      "t = 155.0\n",
      "t = 156.0\n",
      "t = 157.0\n",
      "t = 158.0\n",
      "t = 159.0\n",
      "t = 160.0\n",
      "t = 161.0\n",
      "t = 162.0\n",
      "t = 163.0\n",
      "t = 164.0\n",
      "t = 165.0\n",
      "t = 166.0\n",
      "t = 167.0\n",
      "t = 168.0\n",
      "t = 169.0\n",
      "t = 170.0\n",
      "t = 171.0\n",
      "t = 172.0\n",
      "t = 173.0\n",
      "t = 174.0\n",
      "t = 175.0\n",
      "t = 176.0\n",
      "t = 177.0\n",
      "t = 178.0\n",
      "t = 179.0\n",
      "t = 180.0\n",
      "t = 181.0\n",
      "t = 182.0\n",
      "t = 183.0\n",
      "t = 184.0\n",
      "t = 185.0\n",
      "t = 186.0\n",
      "t = 187.0\n",
      "t = 188.0\n",
      "t = 189.0\n",
      "t = 190.0\n",
      "t = 191.0\n",
      "t = 192.0\n",
      "t = 193.0\n",
      "t = 194.0\n",
      "t = 195.0\n",
      "t = 196.0\n",
      "t = 197.0\n",
      "t = 198.0\n",
      "t = 199.0\n"
     ]
    }
   ],
   "source": [
    "d.visualize_trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.plot_Q_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
